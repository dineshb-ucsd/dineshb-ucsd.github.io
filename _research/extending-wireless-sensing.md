---
title: "New Horizons with Wireless Sensing"
layout: single-portfolio
excerpt: "<img src='/images/research/map.png' alt=''>"
collection: research
order_number: 20
header: 
  og_image: "research/map.png"
---

Extending the Horizon of wireless sensing â€“ Continuum Force Sensing: 

Over the last few years, there has been a significant push to utilize wireless signals for communicating information and sensing the environment and human interactions, even with our research as outlined before. Today, we have wireless sensing capabilities to sense the ambient temperature of the environment, vibrations, hand gestures, and even breathing rate. Force is another such physical phenomenon that enables richer information about both the environment and human interactions. For instance, objects placed on a table exert their gravitational forces, and the contact interactions via our hands/feet are guided by the sense of contact force felt by our skin. Thus, the ability to sense the contact forces can allow us to measure all these ubiquitous interactions, enabling a myriad of applications in AR/VR. However, to date, the rich information that force can give about our ubiquitous environment and the touch interactions have not been captured by wireless sensors. Furthermore, force sensors are a critical requirement for safer surgeries, which require measuring complex contact forces experienced as a surgical instrument interacts with the surrounding tissues during the surgical procedure. 

However, with currently available discrete point-force sensors, which require a battery to sense the forces and communicate the readings wirelessly, these ubiquitous sensing and surgical sensing applications are not practical. This motivates the development of new force sensors that can sense force on a continuum and communicate wirelessly without consuming significant power to enable a battery-free design [Fig. 8]. Breaking away from traditional sensing methods, which have different sensing and communication blocks (eg. MEMS force sensors + BLE communication modules), WiForce, our work unifies these into a joint sensing and communication paradigm that is sufficiently low-powered and can be made potentially battery-less as well. Instead of using hundreds of sensors deployed across the length, WiForce designs a single continuous force-sensitive strip that transduces force magnitude and location onto variations in the reflected signal. These variations in reflected signals translate to variations in the wireless channel measured by any externally located device. Thus, the WiForce sensor can be read by any appropriate waveform capable of channel sounding, like FMCW for radar sensing and OFDM for Wi-Fi.


Autonomous Perception system: Computer Vision meets Wireless Sensing. 

As a faculty, I expanded my research horizon to learn machine learning and computer vision while connecting these topics to wireless. Automated driving requires sensing of the environment and performing actions related to driving based on that. However, performing automated driving primarily depends on how well we can sense the environment around us. Especially, how well can machine (car) see with sensors compared to human eyes, especially under non-line-of-sight, or bad weather conditions fog, rain, snow or even fire?" is extremely important to perform automated driving. I started working on computer vision to understand the limitations of low-cost camera-based sensing. In that context, we developed depth sensing with a camera for long distances in the first project. I combined the space-time coherence ideas from wireless communication with computer vision-based depth prediction and developed an unsupervised depth prediction model which outperformed by 40% compared to state-of-the-art, which was published in CVPR 2019. I was lead faculty author, collaborators from Toyota, my students, and my colleague Javidi. Next, I developed a camera perception model robust to weather and other perturbations in the data yielded interesting and far-reaching results compared to state-of-art published in ECCV 2021.  With the learning from computer vision, I was curious about leveraging and developing radars, which can enable sensing in inclement weather and/or dark, which otherwise is impossible with camera. 
Radars in automotive perception systems provide a low-cost alternative to Lidar, which can work in inclement weather but suffer from other non-idealities. Autonomous perception requires sensors to provide a detailed bounding box of sensed vehicles, objects. However, the radars use wireless signals, which are specular by nature when they reflect off surfaces; thus, making imaging with a detailed bounding box of perceived objects challenging. We have demonstrated for the first time, a distributed radar system with appropriate antenna placement, can overcome the specular reflection, allowing radars to recreate the bounding box in Sensys 2020. We developed AI/computer vision techniques for radar distribution to predict detailed bounding boxes even in bad weather. This work was news and media, as it solved a long-standing question of whether radars can replace Lidar, and was recently published in wall street journal. In my research, we have newly developed camera-radar fusion which can provide detailed attributes of the detected objects as well, besides providing more accurate bounding boxes. Our next directions are to enable cooperative sensing in this domain, where multiple sensors on multiple cars can collaborate. Furthermore, we are developing an end-to-end navigation framework to deduce the requirement of perception, if and when it fails the root cause for the failure, achieving the required perception with a low-cost camera and radar fusion. 
